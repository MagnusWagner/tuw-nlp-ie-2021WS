{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of text processing\n",
    "\n",
    "### Natural Language Processing and Information Extraction,  2021 WS\n",
    "10/15/2021\n",
    "\n",
    "Gábor Recski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this lecture\n",
    "- Regular Expressions\n",
    "\n",
    "- Text segmentation and normalization:\n",
    "   - sentence splitting and tokenization\n",
    "   - lemmatization, stemming, decompounding, morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.3.0-py3-none-any.whl (432 kB)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\magnus\\miniconda3\\lib\\site-packages (from stanza) (1.7.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\magnus\\miniconda3\\lib\\site-packages (from stanza) (4.38.0)\n",
      "Collecting emoji\n",
      "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\magnus\\miniconda3\\lib\\site-packages (from stanza) (1.20.2)\n",
      "Requirement already satisfied: six in c:\\users\\magnus\\miniconda3\\lib\\site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\magnus\\miniconda3\\lib\\site-packages (from stanza) (3.14.0)\n",
      "Requirement already satisfied: requests in c:\\users\\magnus\\miniconda3\\lib\\site-packages (from stanza) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\magnus\\miniconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (3.10.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\magnus\\miniconda3\\lib\\site-packages (from requests->stanza) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\magnus\\miniconda3\\lib\\site-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\magnus\\miniconda3\\lib\\site-packages (from requests->stanza) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\magnus\\miniconda3\\lib\\site-packages (from requests->stanza) (1.26.4)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169294 sha256=faefa3e0cbee465e6f81a0f2e8a324248e4b1b1aa5e48e7fa1acdc11a7a7c40b\n",
      "  Stored in directory: c:\\users\\magnus\\appdata\\local\\pip\\cache\\wheels\\ea\\5f\\d3\\03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-1.6.1 stanza-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\magnus\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\magnus\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\magnus\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing requirements for tensorflow: [Errno 2] No such file or directory: 'c:\\\\users\\\\magnus\\\\miniconda3\\\\lib\\\\site-packages\\\\tensorflow-1.15.0.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for tensorflow-estimator: [Errno 2] No such file or directory: 'c:\\\\users\\\\magnus\\\\miniconda3\\\\lib\\\\site-packages\\\\tensorflow_estimator-1.15.1.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for jdcal: [Errno 2] No such file or directory: 'c:\\\\users\\\\magnus\\\\miniconda3\\\\lib\\\\site-packages\\\\jdcal-1.4.1.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for google-pasta: [Errno 2] No such file or directory: 'c:\\\\users\\\\magnus\\\\miniconda3\\\\lib\\\\site-packages\\\\google_pasta-0.2.0.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for absl-py: [Errno 2] No such file or directory: 'c:\\\\users\\\\magnus\\\\miniconda3\\\\lib\\\\site-packages\\\\absl_py-0.11.0.dist-info\\\\METADATA'\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\magnus\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\magnus\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\magnus\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\magnus\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\magnus\\miniconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Magnus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Magnus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40aad52cb914eb28571c2c380a97ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading https://raw.githubusercontent.com/stanfordnlp/sta…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 13:21:20 INFO: Downloading default packages for language: en (English)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 13:21:21 INFO: File exists: C:\\Users\\Magnus\\stanza_resources\\en\\default.zip.\n",
      "2021-10-15 13:21:24 INFO: Finished downloading models and saved to C:\\Users\\Magnus\\stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb27fe52518f4dceb2889856ace383fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading https://raw.githubusercontent.com/stanfordnlp/sta…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 13:21:24 INFO: Downloading default packages for language: de (German)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 13:21:25 INFO: File exists: C:\\Users\\Magnus\\stanza_resources\\de\\default.zip.\n",
      "2021-10-15 13:21:29 INFO: Finished downloading models and saved to C:\\Users\\Magnus\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import stanza\n",
    "stanza.download('en')\n",
    "stanza.download('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![re1](media/re1.png)([SLP Ch.2](https://web.stanford.edu/~jurafsky/slp3/2.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHAPTER I.\n",
      "Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on\n"
     ]
    }
   ],
   "source": [
    "text = open('data/alice.txt',encoding=\"utf8\").read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(35, 40), match='Alice'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('Alice', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[35:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![re2](media/re2.png)([SLP Ch.2](https://web.stanford.edu/~jurafsky/slp3/2.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(21, 27), match='Rabbit'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('[Rr]abbit', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rabbit', 'Rabbit', 'Rabbit', 'Rabbit', 'rabbit', 'rabbit', 'rabbit']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[Rr]abbit', text[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rabbit (21, 27)\n",
      "Rabbit (589, 595)\n",
      "Rabbit (743, 749)\n",
      "Rabbit (959, 965)\n",
      "rabbit (1149, 1155)\n",
      "rabbit (1341, 1347)\n",
      "rabbit (1486, 1492)\n"
     ]
    }
   ],
   "source": [
    "for match in re.finditer('[Rr]abbit', text[:5000]):\n",
    "    print(match.group(), match.span())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![re3](media/re3.png)([SLP Ch.2](https://web.stanford.edu/~jurafsky/slp3/2.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' the ',\n",
       " ' was ',\n",
       " ' get ',\n",
       " ' her ',\n",
       " ' and ',\n",
       " ' she ',\n",
       " ' her ',\n",
       " ' was ',\n",
       " ' but ',\n",
       " ' had ',\n",
       " ' the ',\n",
       " ' she ',\n",
       " ' her ',\n",
       " ' she ',\n",
       " ' for ',\n",
       " ' day ',\n",
       " ' her ',\n",
       " ' and ',\n",
       " ' the ',\n",
       " ' the ',\n",
       " ' the ',\n",
       " ' was ',\n",
       " ' nor ',\n",
       " ' out ',\n",
       " ' the ',\n",
       " ' the ',\n",
       " ' say ',\n",
       " ' she ',\n",
       " ' her ',\n",
       " ' she ',\n",
       " ' but ',\n",
       " ' all ',\n",
       " ' but ',\n",
       " ' the ',\n",
       " ' out ',\n",
       " ' its ',\n",
       " ' and ',\n",
       " ' and ',\n",
       " ' her ',\n",
       " ' for ',\n",
       " ' her ',\n",
       " ' out ',\n",
       " ' and ',\n",
       " ' she ',\n",
       " ' and ',\n",
       " ' was ',\n",
       " ' see ',\n",
       " ' pop ',\n",
       " ' the ',\n",
       " ' the ',\n",
       " ' she ',\n",
       " ' get ',\n",
       " ' for ',\n",
       " ' and ',\n",
       " ' had ',\n",
       " ' she ',\n",
       " ' the ',\n",
       " ' was ',\n",
       " ' she ',\n",
       " ' for ',\n",
       " ' she ',\n",
       " ' her ',\n",
       " ' she ',\n",
       " ' and ',\n",
       " ' she ',\n",
       " ' but ',\n",
       " ' was ',\n",
       " ' see ',\n",
       " ' the ',\n",
       " ' the ',\n",
       " ' and ',\n",
       " ' and ',\n",
       " ' and ',\n",
       " ' she ',\n",
       " ' and ',\n",
       " ' She ',\n",
       " ' jar ',\n",
       " ' one ',\n",
       " ' the ',\n",
       " ' was ',\n",
       " ' but ',\n",
       " ' her ',\n",
       " ' was ',\n",
       " ' she ',\n",
       " ' not ',\n",
       " ' the ',\n",
       " ' for ',\n",
       " ' put ',\n",
       " ' one ',\n",
       " ' she ',\n",
       " ' How ',\n",
       " ' all ',\n",
       " ' say ',\n",
       " ' off ',\n",
       " ' the ',\n",
       " ' was ',\n",
       " ' the ',\n",
       " ' she ',\n",
       " ' the ',\n",
       " ' the ',\n",
       " ' Let ',\n",
       " ' you ',\n",
       " ' had ',\n",
       " ' her ',\n",
       " ' the ',\n",
       " ' was ',\n",
       " ' for ',\n",
       " ' off ',\n",
       " ' was ',\n",
       " ' one ',\n",
       " ' was ',\n",
       " ' say ',\n",
       " ' the ',\n",
       " ' got ',\n",
       " ' had ',\n",
       " ' but ',\n",
       " ' she ',\n",
       " ' How ',\n",
       " ' out ',\n",
       " ' the ',\n",
       " ' The ',\n",
       " ' was ',\n",
       " ' one ',\n",
       " ' ask ',\n",
       " ' the ',\n",
       " ' you ',\n",
       " ' New ',\n",
       " ' she ',\n",
       " ' she ',\n",
       " ' the ',\n",
       " ' you ',\n",
       " ' you ',\n",
       " ' for ',\n",
       " ' see ',\n",
       " ' was ',\n",
       " ' was ',\n",
       " ' her ',\n",
       " ' you ',\n",
       " ' the ',\n",
       " ' but ',\n",
       " ' and ',\n",
       " ' you ',\n",
       " ' But ',\n",
       " ' eat ',\n",
       " ' And ',\n",
       " ' get ',\n",
       " ' and ',\n",
       " ' eat ',\n",
       " ' eat ',\n",
       " ' eat ',\n",
       " ' you ',\n",
       " ' she ',\n",
       " ' way ',\n",
       " ' put ',\n",
       " ' She ',\n",
       " ' she ',\n",
       " ' and ',\n",
       " ' she ',\n",
       " ' and ',\n",
       " ' her ',\n",
       " ' the ',\n",
       " ' did ',\n",
       " ' eat ',\n",
       " ' she ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(' [A-Za-z][a-z][a-z] ', text[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' the ', 1191),\n",
       " (' and ', 611),\n",
       " (' she ', 348),\n",
       " (' was ', 233),\n",
       " (' you ', 206),\n",
       " (' her ', 154),\n",
       " (' all ', 110),\n",
       " (' had ', 105),\n",
       " (' for ', 105),\n",
       " (' but ', 90)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(re.findall(' [A-Za-z][a-z][a-z] ', text)).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![re4](media/re4.png)([SLP Ch.2](https://web.stanford.edu/~jurafsky/slp3/2.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![re5](media/re5.png)([SLP Ch.2](https://web.stanford.edu/~jurafsky/slp3/2.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![re6](media/re6.png)([SLP Ch.2](https://web.stanford.edu/~jurafsky/slp3/2.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHA',\n",
       " 'PTE',\n",
       " 'R I',\n",
       " 'Dow',\n",
       " 'n t',\n",
       " 'he ',\n",
       " 'Rab',\n",
       " 'bit',\n",
       " '-Ho',\n",
       " 'Ali',\n",
       " 'ce ',\n",
       " 'was',\n",
       " ' be',\n",
       " 'gin',\n",
       " 'nin',\n",
       " 'g t',\n",
       " 'o g',\n",
       " 'et ',\n",
       " 'ver',\n",
       " 'y t',\n",
       " 'ire',\n",
       " 'd o',\n",
       " 'f s',\n",
       " 'itt',\n",
       " 'ing',\n",
       " ' by',\n",
       " ' he',\n",
       " 'r s',\n",
       " 'ist',\n",
       " 'er ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('...', text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![re7](media/re7.png)([SLP Ch.2](https://web.stanford.edu/~jurafsky/slp3/2.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C',\n",
       " 'H',\n",
       " 'A',\n",
       " 'P',\n",
       " 'T',\n",
       " 'E',\n",
       " 'R',\n",
       " 'I',\n",
       " 'D',\n",
       " 'o',\n",
       " 'w',\n",
       " 'n',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'R',\n",
       " 'a',\n",
       " 'b',\n",
       " 'b',\n",
       " 'i',\n",
       " 't',\n",
       " 'H',\n",
       " 'o',\n",
       " 'l',\n",
       " 'e',\n",
       " 'A',\n",
       " 'l',\n",
       " 'i',\n",
       " 'c',\n",
       " 'e',\n",
       " 'w',\n",
       " 'a',\n",
       " 's',\n",
       " 'b',\n",
       " 'e',\n",
       " 'g',\n",
       " 'i',\n",
       " 'n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w', text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'CHAPTER',\n",
       " 'I.',\n",
       " 'Down',\n",
       " 'the',\n",
       " 'Rabbit-Hole',\n",
       " '',\n",
       " '',\n",
       " 'Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s', text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![re8](media/re8.png)([SLP Ch.2](https://web.stanford.edu/~jurafsky/slp3/2.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER',\n",
       " 'I',\n",
       " 'Down',\n",
       " 'the',\n",
       " 'Rabbit',\n",
       " 'Hole',\n",
       " 'Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\w+', text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1533),\n",
       " ('and', 803),\n",
       " ('to', 728),\n",
       " ('a', 617),\n",
       " ('it', 528),\n",
       " ('I', 523),\n",
       " ('she', 510),\n",
       " ('of', 502),\n",
       " ('said', 456),\n",
       " ('Alice', 396),\n",
       " ('in', 356),\n",
       " ('was', 351),\n",
       " ('you', 345),\n",
       " ('that', 274),\n",
       " ('as', 246),\n",
       " ('her', 244),\n",
       " ('t', 216),\n",
       " ('at', 202),\n",
       " ('s', 196),\n",
       " ('on', 189)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(re.findall('\\w+', text)).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 2426),\n",
       " ('“', 1118),\n",
       " ('”', 1114),\n",
       " ('.', 987),\n",
       " ('’', 702),\n",
       " ('!', 451),\n",
       " ('—', 263),\n",
       " (':', 233),\n",
       " ('?', 203),\n",
       " (';', 193),\n",
       " ('-', 142),\n",
       " ('*', 60),\n",
       " ('(', 56),\n",
       " (')', 56),\n",
       " ('‘', 46),\n",
       " ('[', 2),\n",
       " (']', 2)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(re.findall('[^\\w\\s]', text)).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitution and groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\s+', ' ', text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHAPTER\n",
      "I.\n",
      "Down\n",
      "the\n",
      "Rabbit-Hole\n",
      "Alice\n",
      "was\n",
      "beginning\n",
      "to\n",
      "get\n",
      "very\n",
      "tired\n",
      "of\n",
      "sitting\n",
      "by\n",
      "her\n",
      "sister\n",
      "on\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('\\s+', '\\n', text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER I.',\n",
       " 'CHAPTER II.',\n",
       " 'CHAPTER III.',\n",
       " 'CHAPTER IV.',\n",
       " 'CHAPTER V.',\n",
       " 'CHAPTER VI.',\n",
       " 'CHAPTER VII.',\n",
       " 'CHAPTER VIII.',\n",
       " 'CHAPTER IX.',\n",
       " 'CHAPTER X.',\n",
       " 'CHAPTER XI.',\n",
       " 'CHAPTER XII.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('CHAPTER [^\\s]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chapter I.\n",
      "Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('CHAPTER ([^\\s]+)', 'Chapter \\\\1', text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chapter I: Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('CHAPTER ([^\\s.]+).\\n([^\\n]*)', 'Chapter \\\\1: \\\\2', text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall('CHAPTER ([^\\s.]+).\\n([^\\n]*)', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions are surprisingly powerful. Also, with the right implementation, they are literally as fast as you can get. That's because they are equivalent to [finite state automata (FSAs)](https://en.wikipedia.org/wiki/Finite-state_machine). Actually, every regular expression is a [regular grammar](https://en.wikipedia.org/wiki/Regular_grammar) defining a [regular language](https://en.wikipedia.org/wiki/Regular_language)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![re_xkcd](media/re_xkcd.png)([XKCD #208](https://xkcd.com/208/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to split a text into sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"'Of course it's only because Tom isn't home,' said Mrs. Parsons vaguely.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive: split on `.`, `!`, `?`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Of course it's only because Tom isn't home,' said Mrs\",\n",
       " ' Parsons vaguely',\n",
       " '']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('[.!?]', text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better: use language-specific list of abbreviation words, collocations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Of course it's only because Tom isn't home,' said Mrs. Parsons vaguely.\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom lists of patterns are often necessary for special domains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"An die Stelle der Landesgesetze vom 17. Jänner 1883, n.ö.L.G. u. V.Bl. Nr. 35, vom 26. Dezember 1890, n.ö.L.G. u. V.Bl. Nr. 48, vom 17. Juni 1920 n.ö.L.G. u. V.Bl. Nr. 547, vom 4. November 1920 n.ö.L.G. u. V.Bl. Nr. 808, und vom 9. Dezember 1927, L.G.Bl. für Wien Nr. 1 ex 1928, die, soweit dieses Gesetz nichts anderes bestimmt, zugleich ihre Wirksamkeit verlieren, hat die nachfolgende Bauordnung zu treten.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An die Stelle der Landesgesetze vom 17. Jänner 1883, n.ö.L.G. u. V.Bl. Nr. 35, vom 26. Dezember 1890, n.ö.L.G. u. V.Bl. Nr. 48, vom 17. Juni 1920 n.ö.L.G. u. V.Bl. Nr. 547, vom 4. November 1920 n.ö.L.G. u. V.Bl. Nr. 808, und vom 9. Dezember 1927, L.G.Bl. für Wien Nr. 1 ex 1928, die, soweit dieses Gesetz nichts anderes bestimmt, zugleich ihre Wirksamkeit verlieren, hat die nachfolgende Bauordnung zu treten.\n"
     ]
    }
   ],
   "source": [
    "print(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An die Stelle der Landesgesetze vom 17.',\n",
       " 'Jänner 1883, n.ö.L.G.',\n",
       " 'u. V.Bl.',\n",
       " 'Nr. 35, vom 26. Dezember 1890, n.ö.L.G.',\n",
       " 'u. V.Bl.',\n",
       " 'Nr. 48, vom 17. Juni 1920 n.ö.L.G.',\n",
       " 'u. V.Bl.',\n",
       " 'Nr. 547, vom 4. November 1920 n.ö.L.G.',\n",
       " 'u. V.Bl.',\n",
       " 'Nr. 808, und vom 9. Dezember 1927, L.G.Bl.',\n",
       " 'für Wien Nr. 1 ex 1928, die, soweit dieses Gesetz nichts anderes bestimmt, zugleich ihre Wirksamkeit verlieren, hat die nachfolgende Bauordnung zu treten.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(text3, language='german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to  split text into words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive approach: split on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Of\",\n",
       " 'course',\n",
       " \"it's\",\n",
       " 'only',\n",
       " 'because',\n",
       " 'Tom',\n",
       " \"isn't\",\n",
       " \"home,'\",\n",
       " 'said',\n",
       " 'Mrs.',\n",
       " 'Parsons',\n",
       " 'vaguely.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Better: separate punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\",\n",
       " 'Of',\n",
       " 'course',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'only',\n",
       " 'because',\n",
       " 'Tom',\n",
       " 'isn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'home',\n",
       " \",'\",\n",
       " 'said',\n",
       " 'Mrs',\n",
       " '.',\n",
       " 'Parsons',\n",
       " 'vaguely',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('(\\w+|[^\\w\\s]+)', text2)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best: add some language-specific conventions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Of\",\n",
       " 'course',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'only',\n",
       " 'because',\n",
       " 'Tom',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'home',\n",
       " ',',\n",
       " \"'\",\n",
       " 'said',\n",
       " 'Mrs.',\n",
       " 'Parsons',\n",
       " 'vaguely',\n",
       " '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER',\n",
       " 'I',\n",
       " '.',\n",
       " 'Down',\n",
       " 'the',\n",
       " 'Rabbit-Hole',\n",
       " 'Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 2426),\n",
       " ('the', 1520),\n",
       " ('“', 1118),\n",
       " ('”', 1114),\n",
       " ('.', 783),\n",
       " ('and', 774),\n",
       " ('to', 718),\n",
       " ('’', 702),\n",
       " ('a', 611),\n",
       " ('it', 513)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(words).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in words if re.match('\\w', word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1520),\n",
       " ('and', 774),\n",
       " ('to', 718),\n",
       " ('a', 611),\n",
       " ('it', 513),\n",
       " ('I', 511),\n",
       " ('she', 507),\n",
       " ('of', 496),\n",
       " ('said', 453),\n",
       " ('Alice', 396)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(words).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering common function words is called __stopword removal__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yours', 'theirs', 'when', \"isn't\", 'its', 'them', 'd', 'you', \"couldn't\", 'having', 'was', 'those', 'own', 'she', \"shouldn't\", 'myself', 'on', 'up', 'such', 'is', 'against', 'here', 'an', 'their', 'should', 'to', 'in', 'aren', 'into', 'i', 'been', 'hasn', 'whom', 'wasn', 'do', 'between', 'themselves', 'below', \"shan't\", 'have', 'from', \"wouldn't\", \"you'll\", 'ma', 'mustn', 'out', 'doesn', 'few', \"mustn't\", 'has', 'more', 'o', 'a', 'not', 'ourselves', 'under', 'any', 'him', 'over', 'these', 'of', 'off', 'don', 'because', 'that', 'isn', 'above', 've', 'he', 'than', \"weren't\", 's', \"don't\", 'were', 'we', 'which', 'very', \"aren't\", 'again', 'how', 'ain', \"needn't\", 'once', 'mightn', 'won', 'during', 'at', 'shouldn', 'needn', 'can', 'all', 'herself', 'where', 'both', 'did', 'shan', 'each', 'and', 't', 'just', 'yourselves', 'about', \"won't\", \"it's\", 'had', 'her', 'itself', \"she's\", 'himself', 'doing', 'most', 'other', 'same', 'be', 'nor', 'ours', 'but', 'haven', 'then', 'why', \"doesn't\", 'what', 'y', 'm', 'too', 'who', 'by', \"you're\", 'am', 'couldn', 'while', 'no', 'only', 'your', 'some', 'his', 'so', 'our', 'hers', 'now', 'my', 'hadn', 'wouldn', 'will', 'this', 'or', 'with', \"that'll\", 'there', 're', 'through', \"wasn't\", \"you'd\", 'after', 'before', 'weren', 'me', \"hasn't\", 'for', 'are', 'as', 'further', 'll', \"should've\", 'the', 'down', \"hadn't\", \"haven't\", 'being', 'didn', 'does', 'if', 'until', 'they', 'yourself', 'it', \"mightn't\", \"you've\", \"didn't\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in words if word.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 453),\n",
       " ('Alice', 396),\n",
       " ('little', 125),\n",
       " ('one', 93),\n",
       " ('went', 83),\n",
       " ('like', 83),\n",
       " ('thought', 74),\n",
       " ('could', 74),\n",
       " ('Queen', 74),\n",
       " ('know', 72),\n",
       " ('would', 70),\n",
       " ('time', 64),\n",
       " ('see', 64),\n",
       " ('King', 61),\n",
       " ('began', 57),\n",
       " ('Mock', 56),\n",
       " ('Turtle', 56),\n",
       " ('Hatter', 55),\n",
       " ('Gryphon', 55),\n",
       " ('quite', 53)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(words).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and stemming\n",
    "\n",
    "Words like _say_, _says_, and _said_ are all different **word forms** of the same **lemma**. Grouping them together can be useful in many applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming** is the reduction of words to a common prefix, using simple rules that only work some of the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n",
      "say\n",
      "said\n"
     ]
    }
   ],
   "source": [
    "for word in ('say', 'says', 'said'):\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n",
      "hi\n",
      "him\n"
     ]
    }
   ],
   "source": [
    "for word in ('he', 'his', 'him'):\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization** is the mapping of word forms to their lemma, using either a dictionary of word forms, a grammar of how words are formed (a **morphology**), or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 14:26:06 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2021-10-15 14:26:06 INFO: Use device: cpu\n",
      "2021-10-15 14:26:06 INFO: Loading: tokenize\n",
      "2021-10-15 14:26:06 INFO: Loading: pos\n",
      "2021-10-15 14:26:06 INFO: Loading: lemma\n",
      "2021-10-15 14:26:06 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en', processors='tokenize,lemma,pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER\t\tCHAPTER\n",
      "I\t\tI\n",
      ".\t\t.\n",
      "Down\t\tdown\n",
      "the\t\tthe\n",
      "Rabbit-Hole\t\tRabbit-Hole\n",
      "\n",
      "Alice\t\tAlice\n",
      "was\t\tbe\n",
      "beginning\t\tbegin\n",
      "to\t\tto\n",
      "get\t\tget\n",
      "very\t\tvery\n",
      "tired\t\ttired\n",
      "of\t\tof\n",
      "sitting\t\tsit\n",
      "by\t\tby\n",
      "her\t\tshe\n",
      "sister\t\tsister\n",
      "on\t\ton\n",
      "the\t\tthe\n",
      "bank\t\tbank\n",
      ",\t\t,\n",
      "and\t\tand\n",
      "of\t\tof\n",
      "having\t\thave\n",
      "nothing\t\tnothing\n",
      "to\t\tto\n",
      "do\t\tdo\n",
      ":\t\t:\n",
      "once\t\tonce\n",
      "or\t\tor\n",
      "twice\t\ttwice\n",
      "she\t\tshe\n",
      "had\t\thave\n",
      "peeped\t\tpeep\n",
      "into\t\tinto\n",
      "the\t\tthe\n",
      "book\t\tbook\n",
      "her\t\tshe\n",
      "sister\t\tsister\n",
      "was\t\tbe\n",
      "reading\t\tread\n",
      ",\t\t,\n",
      "but\t\tbut\n",
      "it\t\tit\n",
      "had\t\thave\n",
      "no\t\tno\n",
      "pictures\t\tpicture\n",
      "or\t\tor\n",
      "conversations\t\tconversation\n",
      "in\t\tin\n",
      "it\t\tit\n",
      ",\t\t,\n",
      "“\t\t''\n",
      "and\t\tand\n",
      "what\t\twhat\n",
      "is\t\tbe\n",
      "the\t\tthe\n",
      "use\t\tuse\n",
      "of\t\tof\n",
      "a\t\ta\n",
      "book\t\tbook\n",
      ",\t\t,\n",
      "”\t\t''\n",
      "thought\t\tthink\n",
      "Alice\t\tAlice\n",
      "“\t\t''\n",
      "without\t\twithout\n",
      "pictures\t\tpicture\n",
      "or\t\tor\n",
      "conversations\t\tconversation\n",
      "?\t\t?\n",
      "”\t\t''\n",
      "\n",
      "So\t\tso\n",
      "she\t\tshe\n",
      "was\t\tbe\n",
      "considering\t\tconsider\n",
      "in\t\tin\n",
      "her\t\tshe\n",
      "own\t\town\n",
      "mind\t\tmind\n",
      "(\t\t(\n",
      "as\t\tas\n",
      "well\t\twell\n",
      "as\t\tas\n",
      "she\t\tshe\n",
      "could\t\tcould\n",
      ",\t\t,\n",
      "for\t\tfor\n",
      "the\t\tthe\n",
      "hot\t\thot\n",
      "day\t\tday\n",
      "made\t\tmake\n",
      "her\t\tshe\n",
      "feel\t\tfeel\n",
      "very\t\tvery\n",
      "sleepy\t\tsleepy\n",
      "and\t\tand\n",
      "stupid\t\tstupid\n",
      ")\t\t)\n",
      ",\t\t,\n",
      "whether\t\twhether\n",
      "the\t\tthe\n",
      "pleasure\t\tpleasure\n",
      "of\t\tof\n",
      "making\t\tmake\n",
      "a\t\ta\n",
      "daisy\t\tdaisy\n",
      "-\t\t-\n",
      "chain\t\tchain\n",
      "would\t\twould\n",
      "be\t\tbe\n",
      "worth\t\tworth\n",
      "the\t\tthe\n",
      "trouble\t\ttrouble\n",
      "of\t\tof\n",
      "getting\t\tget\n",
      "up\t\tup\n",
      "and\t\tand\n",
      "picking\t\tpick\n",
      "the\t\tthe\n",
      "daisies\t\tdaisies\n",
      ",\t\t,\n",
      "when\t\twhen\n",
      "suddenly\t\tsuddenly\n",
      "a\t\ta\n",
      "White\t\tWhite\n",
      "Rabbit\t\tRabbit\n",
      "with\t\twith\n",
      "pink\t\tpink\n",
      "eyes\t\teye\n",
      "ran\t\trun\n",
      "close\t\tclose\n",
      "by\t\tby\n",
      "her\t\tshe\n",
      ".\t\t.\n",
      "\n",
      "There\t\tthere\n",
      "was\t\tbe\n",
      "nothing\t\tnothing\n",
      "so\t\tso\n",
      "_very_\t\t_very_\n",
      "remarkable\t\tremarkable\n",
      "in\t\tin\n",
      "that\t\tthat\n",
      ";\t\t;\n",
      "nor\t\tnor\n",
      "did\t\tdo\n",
      "Alice\t\tAlice\n",
      "think\t\tthink\n",
      "it\t\tit\n",
      "so\t\tso\n",
      "_very\t\t_very\n",
      "_\t\t_\n",
      "much\t\tmuch\n",
      "out\t\tout\n",
      "of\t\tof\n",
      "the\t\tthe\n",
      "way\t\tway\n",
      "to\t\tto\n",
      "hear\t\thear\n",
      "the\t\tthe\n",
      "Rabbit\t\trabbit\n",
      "say\t\tsay\n",
      "to\t\tto\n",
      "itself\t\titself\n",
      ",\t\t,\n",
      "“\t\t''\n",
      "Oh\t\toh\n",
      "dear\t\tdear\n",
      "!\t\t!\n",
      "\n",
      "Oh\t\toh\n",
      "dear\t\tdear\n",
      "!\t\t!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sentences[:5]:\n",
    "    for word in sentence.words:\n",
    "        print(word.text + '\\t\\t' + word.lemma)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 585),\n",
       " ('say', 526),\n",
       " ('Alice', 395),\n",
       " ('go', 180),\n",
       " ('think', 130),\n",
       " ('little', 126),\n",
       " ('get', 113),\n",
       " ('know', 107),\n",
       " ('look', 105),\n",
       " ('one', 102),\n",
       " ('see', 101),\n",
       " ('come', 97),\n",
       " ('like', 92),\n",
       " ('begin', 91),\n",
       " ('would', 91),\n",
       " ('could', 83),\n",
       " ('thing', 79),\n",
       " ('make', 76),\n",
       " ('time', 73),\n",
       " ('Queen', 73)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(\n",
    "    word.lemma for sentence in doc.sentences for word in sentence.words\n",
    "    if word.lemma not in stopwords and re.match('\\w', word.lemma)).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full analysis of how a word form is built from its lemma is known as **morphological analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER\tCHAPTER\tPROPN\tNumber=Sing\n",
      "I\tI\tPRON\tCase=Nom|Number=Sing|Person=1|PronType=Prs\n",
      ".\t.\tPUNCT\t\n",
      "Down\tdown\tADP\t\n",
      "the\tthe\tDET\tDefinite=Def|PronType=Art\n",
      "Rabbit-Hole\tRabbit-Hole\tPROPN\tNumber=Sing\n",
      "\n",
      "Alice\tAlice\tPROPN\tNumber=Sing\n",
      "was\tbe\tAUX\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "beginning\tbegin\tVERB\tTense=Pres|VerbForm=Part\n",
      "to\tto\tPART\t\n",
      "get\tget\tAUX\tVerbForm=Inf\n",
      "very\tvery\tADV\t\n",
      "tired\ttired\tADJ\tDegree=Pos\n",
      "of\tof\tSCONJ\t\n",
      "sitting\tsit\tVERB\tVerbForm=Ger\n",
      "by\tby\tADP\t\n",
      "her\tshe\tPRON\tGender=Fem|Number=Sing|Person=3|Poss=Yes|PronType=Prs\n",
      "sister\tsister\tNOUN\tNumber=Sing\n",
      "on\ton\tADP\t\n",
      "the\tthe\tDET\tDefinite=Def|PronType=Art\n",
      "bank\tbank\tNOUN\tNumber=Sing\n",
      ",\t,\tPUNCT\t\n",
      "and\tand\tCCONJ\t\n",
      "of\tof\tSCONJ\t\n",
      "having\thave\tVERB\tVerbForm=Ger\n",
      "nothing\tnothing\tPRON\tNumber=Sing\n",
      "to\tto\tPART\t\n",
      "do\tdo\tVERB\tVerbForm=Inf\n",
      ":\t:\tPUNCT\t\n",
      "once\tonce\tADV\tNumType=Mult\n",
      "or\tor\tCCONJ\t\n",
      "twice\ttwice\tSCONJ\t\n",
      "she\tshe\tPRON\tCase=Nom|Gender=Fem|Number=Sing|Person=3|PronType=Prs\n",
      "had\thave\tAUX\tMood=Ind|Tense=Past|VerbForm=Fin\n",
      "peeped\tpeep\tVERB\tTense=Past|VerbForm=Part\n",
      "into\tinto\tADP\t\n",
      "the\tthe\tDET\tDefinite=Def|PronType=Art\n",
      "book\tbook\tNOUN\tNumber=Sing\n",
      "her\tshe\tPRON\tGender=Fem|Number=Sing|Person=3|Poss=Yes|PronType=Prs\n",
      "sister\tsister\tNOUN\tNumber=Sing\n",
      "was\tbe\tAUX\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "reading\tread\tVERB\tTense=Pres|VerbForm=Part\n",
      ",\t,\tPUNCT\t\n",
      "but\tbut\tCCONJ\t\n",
      "it\tit\tPRON\tCase=Nom|Gender=Neut|Number=Sing|Person=3|PronType=Prs\n",
      "had\thave\tVERB\tMood=Ind|Tense=Past|VerbForm=Fin\n",
      "no\tno\tDET\t\n",
      "pictures\tpicture\tNOUN\tNumber=Plur\n",
      "or\tor\tCCONJ\t\n",
      "conversations\tconversation\tNOUN\tNumber=Plur\n",
      "in\tin\tADP\t\n",
      "it\tit\tPRON\tCase=Acc|Gender=Neut|Number=Sing|Person=3|PronType=Prs\n",
      ",\t,\tPUNCT\t\n",
      "“\t''\tPUNCT\t\n",
      "and\tand\tCCONJ\t\n",
      "what\twhat\tPRON\tPronType=Int\n",
      "is\tbe\tAUX\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "the\tthe\tDET\tDefinite=Def|PronType=Art\n",
      "use\tuse\tNOUN\tNumber=Sing\n",
      "of\tof\tADP\t\n",
      "a\ta\tDET\tDefinite=Ind|PronType=Art\n",
      "book\tbook\tNOUN\tNumber=Sing\n",
      ",\t,\tPUNCT\t\n",
      "”\t''\tPUNCT\t\n",
      "thought\tthink\tVERB\tMood=Ind|Tense=Past|VerbForm=Fin\n",
      "Alice\tAlice\tPROPN\tNumber=Sing\n",
      "“\t''\tPUNCT\t\n",
      "without\twithout\tADP\t\n",
      "pictures\tpicture\tNOUN\tNumber=Plur\n",
      "or\tor\tCCONJ\t\n",
      "conversations\tconversation\tNOUN\tNumber=Plur\n",
      "?\t?\tPUNCT\t\n",
      "”\t''\tPUNCT\t\n",
      "\n",
      "So\tso\tADV\t\n",
      "she\tshe\tPRON\tCase=Nom|Gender=Fem|Number=Sing|Person=3|PronType=Prs\n",
      "was\tbe\tAUX\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "considering\tconsider\tVERB\tTense=Pres|VerbForm=Part\n",
      "in\tin\tADP\t\n",
      "her\tshe\tPRON\tGender=Fem|Number=Sing|Person=3|Poss=Yes|PronType=Prs\n",
      "own\town\tADJ\tDegree=Pos\n",
      "mind\tmind\tNOUN\tNumber=Sing\n",
      "(\t(\tPUNCT\t\n",
      "as\tas\tADV\t\n",
      "well\twell\tADV\tDegree=Pos\n",
      "as\tas\tSCONJ\t\n",
      "she\tshe\tPRON\tCase=Nom|Gender=Fem|Number=Sing|Person=3|PronType=Prs\n",
      "could\tcould\tAUX\tVerbForm=Fin\n",
      ",\t,\tPUNCT\t\n",
      "for\tfor\tADP\t\n",
      "the\tthe\tDET\tDefinite=Def|PronType=Art\n",
      "hot\thot\tADJ\tDegree=Pos\n",
      "day\tday\tNOUN\tNumber=Sing\n",
      "made\tmake\tVERB\tMood=Ind|Tense=Past|VerbForm=Fin\n",
      "her\tshe\tPRON\tCase=Acc|Gender=Fem|Number=Sing|Person=3|PronType=Prs\n",
      "feel\tfeel\tVERB\tVerbForm=Inf\n",
      "very\tvery\tADV\t\n",
      "sleepy\tsleepy\tADJ\tDegree=Pos\n",
      "and\tand\tCCONJ\t\n",
      "stupid\tstupid\tADJ\tDegree=Pos\n",
      ")\t)\tPUNCT\t\n",
      ",\t,\tPUNCT\t\n",
      "whether\twhether\tSCONJ\t\n",
      "the\tthe\tDET\tDefinite=Def|PronType=Art\n",
      "pleasure\tpleasure\tNOUN\tNumber=Sing\n",
      "of\tof\tSCONJ\t\n",
      "making\tmake\tVERB\tVerbForm=Ger\n",
      "a\ta\tDET\tDefinite=Ind|PronType=Art\n",
      "daisy\tdaisy\tNOUN\tNumber=Sing\n",
      "-\t-\tPUNCT\t\n",
      "chain\tchain\tNOUN\tNumber=Sing\n",
      "would\twould\tAUX\tVerbForm=Fin\n",
      "be\tbe\tAUX\tVerbForm=Inf\n",
      "worth\tworth\tADJ\tDegree=Pos\n",
      "the\tthe\tDET\tDefinite=Def|PronType=Art\n",
      "trouble\ttrouble\tNOUN\tNumber=Sing\n",
      "of\tof\tSCONJ\t\n",
      "getting\tget\tVERB\tVerbForm=Ger\n",
      "up\tup\tADP\t\n",
      "and\tand\tCCONJ\t\n",
      "picking\tpick\tVERB\tVerbForm=Ger\n",
      "the\tthe\tDET\tDefinite=Def|PronType=Art\n",
      "daisies\tdaisies\tNOUN\tNumber=Plur\n",
      ",\t,\tPUNCT\t\n",
      "when\twhen\tSCONJ\tPronType=Int\n",
      "suddenly\tsuddenly\tADV\t\n",
      "a\ta\tDET\tDefinite=Ind|PronType=Art\n",
      "White\tWhite\tADJ\tDegree=Pos\n",
      "Rabbit\tRabbit\tPROPN\tNumber=Sing\n",
      "with\twith\tADP\t\n",
      "pink\tpink\tADJ\tDegree=Pos\n",
      "eyes\teye\tNOUN\tNumber=Plur\n",
      "ran\trun\tVERB\tMood=Ind|Tense=Past|VerbForm=Fin\n",
      "close\tclose\tADJ\tDegree=Pos\n",
      "by\tby\tADP\t\n",
      "her\tshe\tPRON\tCase=Acc|Gender=Fem|Number=Sing|Person=3|PronType=Prs\n",
      ".\t.\tPUNCT\t\n",
      "\n",
      "There\tthere\tPRON\t\n",
      "was\tbe\tVERB\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "nothing\tnothing\tPRON\tNumber=Sing\n",
      "so\tso\tADV\t\n",
      "_very_\t_very_\tADV\tDegree=Pos\n",
      "remarkable\tremarkable\tADJ\tDegree=Pos\n",
      "in\tin\tADP\t\n",
      "that\tthat\tPRON\tNumber=Sing|PronType=Dem\n",
      ";\t;\tPUNCT\t\n",
      "nor\tnor\tCCONJ\t\n",
      "did\tdo\tAUX\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "Alice\tAlice\tPROPN\tNumber=Sing\n",
      "think\tthink\tVERB\tVerbForm=Inf\n",
      "it\tit\tPRON\tCase=Acc|Gender=Neut|Number=Sing|Person=3|PronType=Prs\n",
      "so\tso\tADV\t\n",
      "_very\t_very\tINTJ\t\n",
      "_\t_\tPUNCT\t\n",
      "much\tmuch\tADV\tDegree=Pos\n",
      "out\tout\tADP\t\n",
      "of\tof\tADP\t\n",
      "the\tthe\tDET\tDefinite=Def|PronType=Art\n",
      "way\tway\tNOUN\tNumber=Sing\n",
      "to\tto\tPART\t\n",
      "hear\thear\tVERB\tVerbForm=Inf\n",
      "the\tthe\tDET\tDefinite=Def|PronType=Art\n",
      "Rabbit\trabbit\tNOUN\tNumber=Sing\n",
      "say\tsay\tVERB\tVerbForm=Inf\n",
      "to\tto\tADP\t\n",
      "itself\titself\tPRON\tCase=Acc|Gender=Neut|Number=Sing|Person=3|PronType=Prs|Reflex=Yes\n",
      ",\t,\tPUNCT\t\n",
      "“\t''\tPUNCT\t\n",
      "Oh\toh\tINTJ\t\n",
      "dear\tdear\tADJ\tDegree=Pos\n",
      "!\t!\tPUNCT\t\n",
      "\n",
      "Oh\toh\tINTJ\t\n",
      "dear\tdear\tINTJ\t\n",
      "!\t!\tPUNCT\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sentences[:5]:\n",
    "    for word in sentence.words:\n",
    "        print('\\t'.join([word.text, word.lemma, word.upos, word.feats if word.feats else '']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special case of lemmatization is **decompounding**, recognizing multiple lemmas in a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"roller-coaster\",\n",
       "      \"lemma\": \"roller-coaster\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 14\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('roller-coaster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"wastebasket\",\n",
       "      \"lemma\": \"wastebasket\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 11\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('wastebasket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For English you might say that this is good enough... but _some languages_ allow forming compounds on the fly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 14:36:53 WARNING: Language de package default expects mwt, which has been added\n",
      "2021-10-15 14:36:53 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-10-15 14:36:53 INFO: Use device: cpu\n",
      "2021-10-15 14:36:53 INFO: Loading: tokenize\n",
      "2021-10-15 14:36:53 INFO: Loading: mwt\n",
      "2021-10-15 14:36:53 INFO: Loading: pos\n",
      "2021-10-15 14:36:53 INFO: Loading: lemma\n",
      "2021-10-15 14:36:53 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_de = stanza.Pipeline('de', processors='tokenize,lemma,pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"Kassenidentifikationsnummer\",\n",
       "      \"lemma\": \"Kassenidentifikationsnummer\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Case=Nom|Gender=Fem|Number=Sing\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 27\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_de('Kassenidentifikationsnummer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no good generic solution and no standard tool. There are some unsupervised approaches like [SECOS](https://github.com/riedlma/SECOS) and [CharSplit](https://github.com/dtuggener/CharSplit), and there are also full-fledged morphological analyzers that might work, like [SMOR](https://www.cis.lmu.de/~schmid/tools/SMOR/) and its extensions [zmorge](https://pub.cl.uzh.ch/users/sennrich/zmorge/) and [SMORLemma](https://github.com/rsennrich/SMORLemma)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing with regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHAPTER I.\n",
      "Down the Rabbit-Hole\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into\n",
      "the book her sister was reading, but it had no pictures or\n",
      "conversations in it, “and what is the use of a book,” thought Alice\n",
      "“without pictures or conversations?”\n",
      "\n",
      "So she was considering in her own mind (as well as she could, for the\n",
      "hot day made her feel very sleepy and stupid), whether the pleasure of\n",
      "making a daisy-chain would be worth the trouble of getting up and\n",
      "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
      "close by her.\n",
      "\n",
      "There was nothing so _very_ remarkable in that; nor did Alice think it\n",
      "so _very_ much out of the way to hear the Rabbit say to itself, “Oh\n",
      "dear! Oh dear! I shall be late!” (when she thought it over afterwards,\n",
      "it occurred to her that she ought to have wondered at this, but at the\n",
      "time it all seemed quite natural); but when the Rabbit actually _took a\n",
      "watch out of its \n"
     ]
    }
   ],
   "source": [
    "text = open('data/alice.txt',encoding=\"utf8\").read()\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub('_','',text)\n",
    "    cleaned_text = re.sub('\\n', ' ', cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CHAPTER I. Down the Rabbit-Hole   Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice “without pictures or conversations?”  So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.  There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, “Oh dear! Oh dear! I shall be late!” (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waist\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split this into sentences, then words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CHAPTER I.\n",
      "\n",
      "Down the Rabbit-Hole   Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice “without pictures or conversations?”  So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n",
      "\n",
      "There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, “Oh dear!\n",
      "\n",
      "Oh dear!\n",
      "\n",
      "I shall be late!” (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n'.join(sens[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = [word_tokenize(sen) for sen in sens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER\n",
      "I\n",
      ".\n",
      "\n",
      "Down\n",
      "the\n",
      "Rabbit-Hole\n",
      "Alice\n",
      "was\n",
      "beginning\n",
      "to\n",
      "get\n",
      "very\n",
      "tired\n",
      "of\n",
      "sitting\n",
      "by\n",
      "her\n",
      "sister\n",
      "on\n",
      "the\n",
      "bank\n",
      ",\n",
      "and\n",
      "of\n",
      "having\n",
      "nothing\n",
      "to\n",
      "do\n",
      ":\n",
      "once\n",
      "or\n",
      "twice\n",
      "she\n",
      "had\n",
      "peeped\n",
      "into\n",
      "the\n",
      "book\n",
      "her\n",
      "sister\n",
      "was\n",
      "reading\n",
      ",\n",
      "but\n",
      "it\n",
      "had\n",
      "no\n",
      "pictures\n",
      "or\n",
      "conversations\n",
      "in\n",
      "it\n",
      ",\n",
      "“\n",
      "and\n",
      "what\n",
      "is\n",
      "the\n",
      "use\n",
      "of\n",
      "a\n",
      "book\n",
      ",\n",
      "”\n",
      "thought\n",
      "Alice\n",
      "“\n",
      "without\n",
      "pictures\n",
      "or\n",
      "conversations\n",
      "?\n",
      "”\n",
      "So\n",
      "she\n",
      "was\n",
      "considering\n",
      "in\n",
      "her\n",
      "own\n",
      "mind\n",
      "(\n",
      "as\n",
      "well\n",
      "as\n",
      "she\n",
      "could\n",
      ",\n",
      "for\n",
      "the\n",
      "hot\n",
      "day\n",
      "made\n",
      "her\n",
      "feel\n",
      "very\n",
      "sleepy\n",
      "and\n",
      "stupid\n",
      ")\n",
      ",\n",
      "whether\n",
      "the\n",
      "pleasure\n",
      "of\n",
      "making\n",
      "a\n",
      "daisy-chain\n",
      "would\n",
      "be\n",
      "worth\n",
      "the\n",
      "trouble\n",
      "of\n",
      "getting\n",
      "up\n",
      "and\n",
      "picking\n",
      "the\n",
      "daisies\n",
      ",\n",
      "when\n",
      "suddenly\n",
      "a\n",
      "White\n",
      "Rabbit\n",
      "with\n",
      "pink\n",
      "eyes\n",
      "ran\n",
      "close\n",
      "by\n",
      "her\n",
      ".\n",
      "\n",
      "There\n",
      "was\n",
      "nothing\n",
      "so\n",
      "very\n",
      "remarkable\n",
      "in\n",
      "that\n",
      ";\n",
      "nor\n",
      "did\n",
      "Alice\n",
      "think\n",
      "it\n",
      "so\n",
      "very\n",
      "much\n",
      "out\n",
      "of\n",
      "the\n",
      "way\n",
      "to\n",
      "hear\n",
      "the\n",
      "Rabbit\n",
      "say\n",
      "to\n",
      "itself\n",
      ",\n",
      "“\n",
      "Oh\n",
      "dear\n",
      "!\n",
      "\n",
      "Oh\n",
      "dear\n",
      "!\n",
      "\n",
      "I\n",
      "shall\n",
      "be\n",
      "late\n",
      "!\n",
      "”\n",
      "(\n",
      "when\n",
      "she\n",
      "thought\n",
      "it\n",
      "over\n",
      "afterwards\n",
      ",\n",
      "it\n",
      "occurred\n",
      "to\n",
      "her\n",
      "that\n",
      "she\n",
      "ought\n",
      "to\n",
      "have\n",
      "wondered\n",
      "at\n",
      "this\n",
      ",\n",
      "but\n",
      "at\n",
      "the\n",
      "time\n",
      "it\n",
      "all\n",
      "seemed\n",
      "quite\n",
      "natural\n",
      ")\n",
      ";\n",
      "but\n",
      "when\n",
      "the\n",
      "Rabbit\n",
      "actually\n",
      "took\n",
      "a\n",
      "watch\n",
      "out\n",
      "of\n",
      "its\n",
      "waistcoat-pocket\n",
      ",\n",
      "and\n",
      "looked\n",
      "at\n",
      "it\n",
      ",\n",
      "and\n",
      "then\n",
      "hurried\n",
      "on\n",
      ",\n",
      "Alice\n",
      "started\n",
      "to\n",
      "her\n",
      "feet\n",
      ",\n",
      "for\n",
      "it\n",
      "flashed\n",
      "across\n",
      "her\n",
      "mind\n",
      "that\n",
      "she\n",
      "had\n",
      "never\n",
      "before\n",
      "seen\n",
      "a\n",
      "rabbit\n",
      "with\n",
      "either\n",
      "a\n",
      "waistcoat-pocket\n",
      ",\n",
      "or\n",
      "a\n",
      "watch\n",
      "to\n",
      "take\n",
      "out\n",
      "of\n",
      "it\n",
      ",\n",
      "and\n",
      "burning\n",
      "with\n",
      "curiosity\n",
      ",\n",
      "she\n",
      "ran\n",
      "across\n",
      "the\n",
      "field\n",
      "after\n",
      "it\n",
      ",\n",
      "and\n",
      "fortunately\n",
      "was\n",
      "just\n",
      "in\n",
      "time\n",
      "to\n",
      "see\n",
      "it\n",
      "pop\n",
      "down\n",
      "a\n",
      "large\n",
      "rabbit-hole\n",
      "under\n",
      "the\n",
      "hedge\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n'.join('\\n'.join(sen) for sen in toks[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write this to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/alice_tok.txt', 'w') as f:\n",
    "    f.write('\\n\\n'.join('\\n'.join(sen) for sen in toks) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find all names using regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names(toks):\n",
    "    curr_name = []\n",
    "    for sen in toks:\n",
    "        for tok in sen[1:]:\n",
    "            if re.match('[A-Z][a-z]+', tok):\n",
    "                curr_name.append(tok)\n",
    "            elif curr_name:\n",
    "                yield ' '.join(curr_name)\n",
    "                curr_name = []\n",
    "                \n",
    "        if curr_name:\n",
    "            yield curr_name\n",
    "            \n",
    "        \n",
    "def count_names(toks):\n",
    "    name_counter = Counter()\n",
    "    \n",
    "    for name in find_names(toks):\n",
    "        name_counter[name] += 1\n",
    "    \n",
    "    for name, count in name_counter.most_common():\n",
    "        print(name, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice 341\n",
      "Queen 67\n",
      "King 61\n",
      "Gryphon 54\n",
      "Hatter 53\n",
      "Mock Turtle 53\n",
      "And 51\n",
      "You 48\n",
      "It 46\n",
      "What 40\n",
      "Duchess 40\n",
      "Dormouse 38\n",
      "March Hare 30\n",
      "But 29\n",
      "Mouse 26\n",
      "That 25\n",
      "Caterpillar 25\n",
      "Oh 24\n",
      "The 24\n",
      "Well 23\n",
      "White Rabbit 22\n",
      "Cat 22\n",
      "She 21\n",
      "Rabbit 20\n",
      "How 20\n",
      "Why 20\n",
      "Come 16\n",
      "If 16\n",
      "There 16\n",
      "They 14\n",
      "He 14\n",
      "Then 14\n",
      "So 13\n",
      "Dinah 13\n",
      "Dodo 13\n",
      "Bill 13\n",
      "No 12\n",
      "As 12\n",
      "Yes 12\n",
      "We 11\n",
      "Of 11\n",
      "Pigeon 11\n",
      "Majesty 11\n",
      "Do 9\n",
      "Now 9\n",
      "For 9\n",
      "Who 9\n",
      "Not 9\n",
      "This 9\n",
      "Off 8\n",
      "Lory 7\n",
      "In 7\n",
      "Footman 7\n",
      "Five 7\n",
      "The Queen 7\n",
      "Knave 7\n",
      "Which 6\n",
      "Just 6\n",
      "When 6\n",
      "Lizard 6\n",
      "Hearts 6\n",
      "Soup 6\n",
      "English 5\n",
      "Would 5\n",
      "Are 5\n",
      "Let 5\n",
      "Here 5\n",
      "Two 5\n",
      "Seven 5\n",
      "Soo—oop 5\n",
      "Mabel 4\n",
      "With 4\n",
      "The Mouse 4\n",
      "French 4\n",
      "One 4\n",
      "Said 4\n",
      "Please 4\n",
      "Ah 4\n",
      "Hold 4\n",
      "Sure 4\n",
      "Father William 4\n",
      "At 4\n",
      "Don 4\n",
      "Cheshire Cat 4\n",
      "Call 4\n",
      "Very 4\n",
      "Nothing 4\n",
      "Perhaps 3\n",
      "William 3\n",
      "Duck 3\n",
      "Eaglet 3\n",
      "Did 3\n",
      "Everybody 3\n",
      "Only 3\n",
      "Mary Ann 3\n",
      "Is 3\n",
      "Yet 3\n",
      "Pray 3\n",
      "Serpent 3\n",
      "While 3\n",
      "All 3\n",
      "Have 3\n",
      "Exactly 3\n",
      "Time 3\n",
      "Take 3\n",
      "Consider 3\n",
      "Tis 3\n",
      "Never 3\n",
      "Thank 3\n",
      "Lobster Quadrille 3\n",
      "Owl 3\n",
      "Beautiful 3\n",
      "Give 3\n",
      "Latitude 2\n",
      "Longitude 2\n",
      "Ma 2\n",
      "Paris 2\n",
      "Morcar 2\n",
      "Mercia 2\n",
      "Found 2\n",
      "Fury 2\n",
      "Crab 2\n",
      "On 2\n",
      "Nobody 2\n",
      "The Duchess 2\n",
      "By 2\n",
      "Pat 2\n",
      "An 2\n",
      "Where 2\n",
      "After 2\n",
      "Explain 2\n",
      "Keep 2\n",
      "Can 2\n",
      "Fish-Footman 2\n",
      "Cheshire 2\n",
      "Wow 2\n",
      "May 2\n",
      "Suppose 2\n",
      "The Hatter 2\n",
      "Does 2\n",
      "The Dormouse 2\n",
      "Twinkle 2\n",
      "Up 2\n",
      "Wake 2\n",
      "Tell 2\n",
      "Once 2\n",
      "Treacle 2\n",
      "Really 2\n",
      "Miss 2\n",
      "My 2\n",
      "Turn 2\n",
      "The Knave 2\n",
      "Get 2\n",
      "These 2\n",
      "Tortoise 2\n",
      "Uglification 2\n",
      "So Alice 2\n",
      "Go 2\n",
      "Panther 2\n",
      "Beautiful Soup 2\n",
      "Silence 2\n",
      "First 2\n",
      "Unimportant 2\n",
      "Adventures 2\n",
      "Wonderland 2\n",
      "Rabbit-Hole Alice 1\n",
      "Antipathies 1\n",
      "New Zealand 1\n",
      "Australia 1\n",
      "Down 1\n",
      "Drink 1\n",
      "Soon 1\n",
      "Pool 1\n",
      "Tears 1\n",
      "Curiouser 1\n",
      "Christmas. 1\n",
      "Right Foot 1\n",
      "Esq. 1\n",
      "Hearthrug 1\n",
      "Fender 1\n",
      "The Rabbit 1\n",
      "Dear 1\n",
      "Ada 1\n",
      "Multiplication Table 1\n",
      "Geography 1\n",
      "Rome 1\n",
      "Rome—no 1\n",
      "Improve 1\n",
      "Nile On 1\n",
      "Latin Grammar 1\n",
      "Conqueror. 1\n",
      "Caucus-Race 1\n",
      "Long Tale They 1\n",
      "Sit 1\n",
      "Ahem 1\n",
      "Conqueror 1\n",
      "Northumbria— 1\n",
      "Ugh 1\n",
      "Edwin 1\n",
      "Northumbria 1\n",
      "Stigand 1\n",
      "Canterbury 1\n",
      "Edgar Atheling 1\n",
      "Normans— 1\n",
      "Speak English 1\n",
      "Caucus-race. 1\n",
      "Caucus-race 1\n",
      "Shakespeare 1\n",
      "Prizes 1\n",
      "Hand 1\n",
      "Mine 1\n",
      "Such 1\n",
      "Magpie 1\n",
      "Canary 1\n",
      "Rabbit Sends 1\n",
      "Little Bill It 1\n",
      "And Alice 1\n",
      "Miss Alice 1\n",
      "Coming 1\n",
      "Alas 1\n",
      "Luckily 1\n",
      "Ann 1\n",
      "Fetch 1\n",
      "Digging 1\n",
      "Sounds 1\n",
      "Shy 1\n",
      "Catch 1\n",
      "Last 1\n",
      "Jack-in-the-box 1\n",
      "Poor 1\n",
      "Advice 1\n",
      "Caterpillar The Caterpillar 1\n",
      "Repeat 1\n",
      "Allow 1\n",
      "Has 1\n",
      "Whoever 1\n",
      "Pepper For 1\n",
      "The Frog-Footman 1\n",
      "From 1\n",
      "The Footman 1\n",
      "Anything 1\n",
      "Talking 1\n",
      "Twenty-four 1\n",
      "Speak 1\n",
      "Because 1\n",
      "Cheshire Puss 1\n",
      "To 1\n",
      "By-the-bye 1\n",
      "March. 1\n",
      "Mad Tea-Party There 1\n",
      "Your 1\n",
      "The March Hare 1\n",
      "Nor 1\n",
      "March—just 1\n",
      "Like 1\n",
      "Elsie 1\n",
      "Lacie 1\n",
      "Tillie 1\n",
      "Sh 1\n",
      "Croquet-Ground 1\n",
      "Look 1\n",
      "Kings 1\n",
      "Queens 1\n",
      "Idiot 1\n",
      "Nonsense 1\n",
      "Leave 1\n",
      "Their 1\n",
      "Hush 1\n",
      "Story 1\n",
      "Tut 1\n",
      "Everything 1\n",
      "Somebody 1\n",
      "Birds 1\n",
      "Right 1\n",
      "Be 1\n",
      "Thinking 1\n",
      "Those 1\n",
      "Mock Turtle Soup 1\n",
      "Turtle. 1\n",
      "Hjckrrh 1\n",
      "Turtle—we 1\n",
      "Tortoise— 1\n",
      "Drive 1\n",
      "Certainly 1\n",
      "Reeling 1\n",
      "Writhing 1\n",
      "Arithmetic—Ambition 1\n",
      "Distraction 1\n",
      "Derision. 1\n",
      "The Gryphon 1\n",
      "Mystery 1\n",
      "Seaography 1\n",
      "Drawling—the Drawling-master 1\n",
      "Drawling 1\n",
      "Stretching 1\n",
      "Fainting 1\n",
      "Coils. 1\n",
      "Hadn 1\n",
      "Classics 1\n",
      "Laughing 1\n",
      "Grief 1\n",
      "Ten 1\n",
      "Lobster Quadrille The Mock Turtle 1\n",
      "Same 1\n",
      "Seals 1\n",
      "Each 1\n",
      "Swim 1\n",
      "Change 1\n",
      "Back 1\n",
      "Will 1\n",
      "Too 1\n",
      "England 1\n",
      "France— Then 1\n",
      "Dinn 1\n",
      "Boots 1\n",
      "Soles 1\n",
      "Wouldn 1\n",
      "Stand 1\n",
      "However 1\n",
      "Lobster 1\n",
      "Trims 1\n",
      "Shark 1\n",
      "His 1\n",
      "The Panther 1\n",
      "Was 1\n",
      "Shall 1\n",
      "Or 1\n",
      "Hm 1\n",
      "Turtle Soup 1\n",
      "The Mock Turtle 1\n",
      "Waiting 1\n",
      "Game 1\n",
      "Chorus 1\n",
      "Stole 1\n",
      "Tarts 1\n",
      "Stupid 1\n",
      "Herald 1\n",
      "Fourteenth 1\n",
      "March 1\n",
      "Fifteenth 1\n",
      "Sixteenth 1\n",
      "Write 1\n",
      "Stolen 1\n",
      "Bring 1\n",
      "Shan 1\n",
      "Your Majesty 1\n",
      "Pepper 1\n",
      "Collar 1\n",
      "Behead 1\n",
      "Evidence 1\n",
      "Rule Forty-two 1\n",
      "Nearly 1\n",
      "Number One 1\n",
      "Read 1\n",
      "Begin 1\n",
      "Though 1\n",
      "Involved 1\n",
      "Before 1\n",
      "Him 1\n",
      "Between 1\n",
      "Sentence 1\n",
      "Stuff 1\n"
     ]
    }
   ],
   "source": [
    "count_names(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter our tokens for stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_without_stopwords = [[tok for tok in sen if tok.lower() not in stopwords] for sen in toks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER\n",
      ".\n",
      "\n",
      "Rabbit-Hole\n",
      "Alice\n",
      "beginning\n",
      "get\n",
      "tired\n",
      "sitting\n",
      "sister\n",
      "bank\n",
      ",\n",
      "nothing\n",
      ":\n",
      "twice\n",
      "peeped\n",
      "book\n",
      "sister\n",
      "reading\n",
      ",\n",
      "pictures\n",
      "conversations\n",
      ",\n",
      "“\n",
      "use\n",
      "book\n",
      ",\n",
      "”\n",
      "thought\n",
      "Alice\n",
      "“\n",
      "without\n",
      "pictures\n",
      "conversations\n",
      "?\n",
      "”\n",
      "considering\n",
      "mind\n",
      "(\n",
      "well\n",
      "could\n",
      ",\n",
      "hot\n",
      "day\n",
      "made\n",
      "feel\n",
      "sleepy\n",
      "stupid\n",
      ")\n",
      ",\n",
      "whether\n",
      "pleasure\n",
      "making\n",
      "daisy-chain\n",
      "would\n",
      "worth\n",
      "trouble\n",
      "getting\n",
      "picking\n",
      "daisies\n",
      ",\n",
      "suddenly\n",
      "White\n",
      "Rabbit\n",
      "pink\n",
      "eyes\n",
      "ran\n",
      "close\n",
      ".\n",
      "\n",
      "nothing\n",
      "remarkable\n",
      ";\n",
      "Alice\n",
      "think\n",
      "much\n",
      "way\n",
      "hear\n",
      "Rabbit\n",
      "say\n",
      ",\n",
      "“\n",
      "Oh\n",
      "dear\n",
      "!\n",
      "\n",
      "Oh\n",
      "dear\n",
      "!\n",
      "\n",
      "shall\n",
      "late\n",
      "!\n",
      "”\n",
      "(\n",
      "thought\n",
      "afterwards\n",
      ",\n",
      "occurred\n",
      "ought\n",
      "wondered\n",
      ",\n",
      "time\n",
      "seemed\n",
      "quite\n",
      "natural\n",
      ")\n",
      ";\n",
      "Rabbit\n",
      "actually\n",
      "took\n",
      "watch\n",
      "waistcoat-pocket\n",
      ",\n",
      "looked\n",
      ",\n",
      "hurried\n",
      ",\n",
      "Alice\n",
      "started\n",
      "feet\n",
      ",\n",
      "flashed\n",
      "across\n",
      "mind\n",
      "never\n",
      "seen\n",
      "rabbit\n",
      "either\n",
      "waistcoat-pocket\n",
      ",\n",
      "watch\n",
      "take\n",
      ",\n",
      "burning\n",
      "curiosity\n",
      ",\n",
      "ran\n",
      "across\n",
      "field\n",
      ",\n",
      "fortunately\n",
      "time\n",
      "see\n",
      "pop\n",
      "large\n",
      "rabbit-hole\n",
      "hedge\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n'.join('\\n'.join(sen) for sen in toks_without_stopwords[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice 342\n",
      "Queen 67\n",
      "King 55\n",
      "Mock Turtle 51\n",
      "Gryphon 51\n",
      "Hatter 49\n",
      "Duchess 39\n",
      "Dormouse 35\n",
      "March Hare 29\n",
      "Mouse 28\n",
      "Rabbit 24\n",
      "Oh 24\n",
      "Caterpillar 24\n",
      "Well 23\n",
      "Cat 19\n",
      "White Rabbit 17\n",
      "Come 16\n",
      "Dinah 13\n",
      "Dodo 12\n",
      "Yes 12\n",
      "Bill 12\n",
      "Pigeon 11\n",
      "Majesty 11\n",
      "Footman 8\n",
      "Lory 6\n",
      "Lizard 6\n",
      "Five 6\n",
      "Soup 6\n",
      "English 5\n",
      "Would 5\n",
      "Let 5\n",
      "Two 5\n",
      "Knave 5\n",
      "Soo—oop 5\n",
      "Mabel 4\n",
      "French 4\n",
      "One 4\n",
      "Said 4\n",
      "Ah 4\n",
      "Hold 4\n",
      "Sure 4\n",
      "Father William 4\n",
      "Cheshire Cat 4\n",
      "Call 4\n",
      "Seven 4\n",
      "Nothing 4\n",
      "Perhaps 3\n",
      "Everybody 3\n",
      "Please 3\n",
      "Mary Ann 3\n",
      "Yet 3\n",
      "Pray 3\n",
      "Serpent 3\n",
      "Exactly 3\n",
      "Time 3\n",
      "Queen Hearts 3\n",
      "Take 3\n",
      "Knave Hearts 3\n",
      "Consider 3\n",
      "Tis 3\n",
      "Never 3\n",
      "Thank 3\n",
      "Lobster Quadrille 3\n",
      "Beautiful 3\n",
      "Give 3\n",
      "Paris 2\n",
      "Found 2\n",
      "Duck 2\n",
      "Eaglet 2\n",
      "Fury 2\n",
      "Crab 2\n",
      "Nobody 2\n",
      "Pat 2\n",
      "Explain 2\n",
      "Keep 2\n",
      "Pepper 2\n",
      "Cheshire 2\n",
      "Wow 2\n",
      "May 2\n",
      "Suppose 2\n",
      "Twinkle 2\n",
      "Wake 2\n",
      "Tell 2\n",
      "Treacle 2\n",
      "Really 2\n",
      "Miss 2\n",
      "Turn 2\n",
      "Get 2\n",
      "Turtle 2\n",
      "Tortoise 2\n",
      "Uglification 2\n",
      "Go 2\n",
      "Panther 2\n",
      "Owl 2\n",
      "Beautiful Soup 2\n",
      "Silence 2\n",
      "First 2\n",
      "Unimportant 2\n",
      "Adventures 2\n",
      "Wonderland 2\n",
      "Latitude Longitude 1\n",
      "Latitude 1\n",
      "Longitude 1\n",
      "New Zealand Australia 1\n",
      "Drink 1\n",
      "Soon 1\n",
      "Tears 1\n",
      "Curiouser 1\n",
      "Christmas. 1\n",
      "Right Foot 1\n",
      "Esq. 1\n",
      "Hearthrug 1\n",
      "Fender 1\n",
      "Dear 1\n",
      "Ada 1\n",
      "Multiplication Table 1\n",
      "Geography 1\n",
      "Rome 1\n",
      "Rome—no 1\n",
      "Improve 1\n",
      "Nile 1\n",
      "Latin Grammar 1\n",
      "William Conqueror. 1\n",
      "Duck Dodo 1\n",
      "Lory Eaglet 1\n",
      "Long Tale 1\n",
      "Sit 1\n",
      "Ahem 1\n",
      "William Conqueror 1\n",
      "Morcar 1\n",
      "Mercia Northumbria— 1\n",
      "Ugh 1\n",
      "Edwin Morcar 1\n",
      "Mercia Northumbria 1\n",
      "Stigand 1\n",
      "Canterbury 1\n",
      "Edgar Atheling 1\n",
      "William 1\n",
      "Normans— 1\n",
      "Speak English 1\n",
      "Caucus-race. 1\n",
      "Caucus-race 1\n",
      "Shakespeare 1\n",
      "Prizes 1\n",
      "Hand 1\n",
      "Mine 1\n",
      "Mouse Alice 1\n",
      "Magpie 1\n",
      "Canary 1\n",
      "Sends Little Bill White Rabbit 1\n",
      "Miss Alice 1\n",
      "Coming 1\n",
      "Alas 1\n",
      "Luckily Alice 1\n",
      "Ann 1\n",
      "Fetch 1\n",
      "Digging 1\n",
      "Sounds 1\n",
      "Shy 1\n",
      "Catch 1\n",
      "Last 1\n",
      "Jack-in-the-box 1\n",
      "Poor 1\n",
      "Advice Caterpillar Caterpillar Alice 1\n",
      "Repeat 1\n",
      "Allow 1\n",
      "Whoever 1\n",
      "Frog-Footman 1\n",
      "Fish-Footman 1\n",
      "Anything 1\n",
      "Talking 1\n",
      "Twenty-four 1\n",
      "Speak 1\n",
      "Cheshire Puss 1\n",
      "By-the-bye 1\n",
      "March. 1\n",
      "Tea-Party 1\n",
      "March Hare Hatter 1\n",
      "March—just 1\n",
      "Like 1\n",
      "Elsie 1\n",
      "Lacie 1\n",
      "Tillie 1\n",
      "Hatter March Hare 1\n",
      "Sh 1\n",
      "Croquet-Ground 1\n",
      "Look 1\n",
      "Five Seven 1\n",
      "Kings Queens 1\n",
      "Idiot 1\n",
      "Nonsense 1\n",
      "Leave 1\n",
      "Hush 1\n",
      "Story 1\n",
      "Tut 1\n",
      "Everything 1\n",
      "Somebody 1\n",
      "Birds 1\n",
      "Right 1\n",
      "Thinking 1\n",
      "Mock Turtle Soup 1\n",
      "Turtle. 1\n",
      "Hjckrrh 1\n",
      "Turtle—we 1\n",
      "Tortoise— 1\n",
      "Drive 1\n",
      "Certainly 1\n",
      "Reeling Writhing 1\n",
      "Arithmetic—Ambition 1\n",
      "Distraction 1\n",
      "Derision. 1\n",
      "Mystery 1\n",
      "Seaography 1\n",
      "Drawling—the Drawling-master 1\n",
      "Drawling 1\n",
      "Stretching 1\n",
      "Fainting Coils. 1\n",
      "Classics 1\n",
      "Laughing Grief 1\n",
      "Ten 1\n",
      "Quadrille Mock Turtle 1\n",
      "Seals 1\n",
      "Swim 1\n",
      "Change 1\n",
      "Back 1\n",
      "Mock Turtle Gryphon 1\n",
      "France— 1\n",
      "Dinn 1\n",
      "Boots 1\n",
      "Soles 1\n",
      "Stand 1\n",
      "However 1\n",
      "Lobster 1\n",
      "Trims 1\n",
      "Shark 1\n",
      "Owl Panther 1\n",
      "Shall 1\n",
      "Hm 1\n",
      "Turtle Soup 1\n",
      "Waiting 1\n",
      "Game 1\n",
      "Chorus 1\n",
      "Tarts 1\n",
      "King White Rabbit 1\n",
      "Stupid 1\n",
      "Herald 1\n",
      "Fourteenth March 1\n",
      "Fifteenth 1\n",
      "Sixteenth 1\n",
      "Write 1\n",
      "Stolen 1\n",
      "Bring 1\n",
      "Collar Dormouse 1\n",
      "Behead Dormouse 1\n",
      "Evidence 1\n",
      "Rule Forty-two 1\n",
      "Nearly 1\n",
      "Number One 1\n",
      "Please Majesty 1\n",
      "Read 1\n",
      "Begin 1\n",
      "Though 1\n",
      "Involved 1\n",
      "Sentence 1\n",
      "Stuff 1\n"
     ]
    }
   ],
   "source": [
    "count_names(toks_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write the stopwords into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stopwords.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(stopwords)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue to [Text processing on the Linux command line](https://github.com/tuw-nlp-ie/tuw-nlp-ie-2021WS/blob/main/lectures/01_Text_processing/01b_Text_processing_Linux_command_line.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
